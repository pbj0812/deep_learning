{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING PYTORCH WITH EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 : [링크](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "- Author : [Justin Johnson](https://github.com/jcjohnson/pytorch-examples)\n",
    "- 번역 & 주석 추가 : [박범진](https://github.com/pbj0812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Defining new autograd functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 원시 autograd 연산자는 Tensor들에 대해서 작동하는 함수들이다.<br>\n",
    "forward 함수는 입력 Tensor를 이용하여 출력 Tensor를 계산한다.<br>\n",
    "backward 함수는 Tensor에 대한 gradient를 받는다.\n",
    "<br>\n",
    "<br>\n",
    "Pytorch에서 torch.autograd.Function의 정의 및 forward와 backward 함수의 구현을 통해<br> \n",
    "쉽게 autograd operator를 정의할 수 있다. 우리는 그것을 불러오고 인스턴스를 생성하고 <br>\n",
    "입력 데이터가 포함된 Tensor를 전달함으로써 새로운 autograd operator를 사용할 수 있다.\n",
    "<br>\n",
    "<br>\n",
    "이 예제에서 우리는 비선형의 ReLU를 만들기 위한 우리의 커스텀 autograd를 정의하고,<br> \n",
    "two-layer 네트워크를 구현하는데에 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    우리는 torch.autograd.Function의 하위분류 및 Tensor에서 작동되는 \n",
    "    forward와 backward의 구현을 통해 우리의 커스텀 autograd 함수를 만들 수 있다.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        forward에서 우리는 입력값과 출력값을 가지는 Tensor를 얻는다. \n",
    "        ctx는 backward 계산을 위한 은닉 정보로 사용되는 객체이다.\n",
    "        ctx.save_for_backward 방법을 사용해서 backward 계산을 위한\n",
    "        임의의 객체를 숨길 수 있다.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        backward 과정에서 우리는 출력값에 관한 loss의 gradient를 포함하는 Tensor를 얻을 수 있다, \n",
    "        그리고 우리는 입력값에 대한 loss의 gradient를 계산 할 필요가 있다.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력값과 출력값을 가지는 random Tensor 생성\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# random wights를 가지는 Tensor 생성\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28359880.0\n",
      "100 318.4908142089844\n",
      "200 1.4441989660263062\n",
      "300 0.01257424708455801\n",
      "400 0.0003245252009946853\n",
      "500 5.221130413701758e-05\n",
      "600 2.0720131942653097e-05\n",
      "700 1.1750641533581074e-05\n",
      "800 7.879493750806432e-06\n",
      "900 5.752538527303841e-06\n",
      "1000 4.511611678026384e-06\n",
      "1100 3.6444782836042577e-06\n",
      "1200 3.0529231480613817e-06\n",
      "1300 2.560029997766833e-06\n",
      "1400 2.2545702904608333e-06\n",
      "1500 1.947849114003475e-06\n",
      "1600 1.705643740024243e-06\n",
      "1700 1.5552759577985853e-06\n",
      "1800 1.4023765970705426e-06\n",
      "1900 1.2828525086661102e-06\n",
      "2000 1.1786255527113099e-06\n",
      "2100 1.0955524203382083e-06\n",
      "2200 9.986133591155522e-07\n",
      "2300 9.239444125341834e-07\n",
      "2400 8.66971276991535e-07\n",
      "2500 8.2318501881673e-07\n",
      "2600 7.620055271218007e-07\n",
      "2700 7.427700552398164e-07\n",
      "2800 6.898133051436162e-07\n",
      "2900 6.461937118729111e-07\n",
      "3000 6.094330728956265e-07\n",
      "3100 5.715688757845783e-07\n",
      "3200 5.697712595065241e-07\n",
      "3300 5.208008246881946e-07\n",
      "3400 5.071437954029534e-07\n",
      "3500 4.785377427651838e-07\n",
      "3600 4.6707421574865293e-07\n",
      "3700 4.4087587980357057e-07\n",
      "3800 4.328552165588917e-07\n",
      "3900 4.2592324689394445e-07\n",
      "4000 3.96763368826214e-07\n",
      "4100 3.8548637348867487e-07\n",
      "4200 3.7686066889364156e-07\n",
      "4300 3.6795171354242484e-07\n",
      "4400 3.625984561494988e-07\n",
      "4500 3.302847062514047e-07\n",
      "4600 3.23460938034259e-07\n",
      "4700 3.011726334989362e-07\n",
      "4800 2.8923182071594056e-07\n",
      "4900 2.729941570578376e-07\n",
      "5000 2.692808607207553e-07\n",
      "5100 2.741293769759068e-07\n",
      "5200 2.7051009965362027e-07\n",
      "5300 2.51910478255013e-07\n",
      "5400 2.601691448944621e-07\n",
      "5500 2.6126488705813244e-07\n",
      "5600 2.484513288436574e-07\n",
      "5700 2.326528090179636e-07\n",
      "5800 2.3138819926771248e-07\n",
      "5900 2.3128386317239347e-07\n",
      "6000 2.317428737796945e-07\n",
      "6100 2.405575116881664e-07\n",
      "6200 2.3000809790119092e-07\n",
      "6300 2.1302206221207598e-07\n",
      "6400 2.112396089160029e-07\n",
      "6500 2.1725684007378732e-07\n",
      "6600 2.0807944167700043e-07\n",
      "6700 2.0820604618165817e-07\n",
      "6800 2.0387507504437963e-07\n",
      "6900 2.0899993558032293e-07\n",
      "7000 2.0361579800010077e-07\n",
      "7100 1.938853984029265e-07\n",
      "7200 1.9606645196290629e-07\n",
      "7300 1.802537497042067e-07\n",
      "7400 1.828589262231617e-07\n",
      "7500 1.648994327752007e-07\n",
      "7600 1.6416238679539674e-07\n",
      "7700 1.6662924906540866e-07\n",
      "7800 1.6060209873103304e-07\n",
      "7900 1.579617077140938e-07\n",
      "8000 1.523349624221737e-07\n",
      "8100 1.5409077036565577e-07\n",
      "8200 1.496172927772932e-07\n",
      "8300 1.5001045028384397e-07\n",
      "8400 1.4444434270899364e-07\n",
      "8500 1.4198892017702747e-07\n",
      "8600 1.411023760056196e-07\n",
      "8700 1.3634785034355446e-07\n",
      "8800 1.3744822524586198e-07\n",
      "8900 1.2772825641604868e-07\n",
      "9000 1.2966997076091502e-07\n",
      "9100 1.2977308472272853e-07\n",
      "9200 1.2683476313668507e-07\n",
      "9300 1.2833221774144477e-07\n",
      "9400 1.2723580766760278e-07\n",
      "9500 1.4050790753117326e-07\n",
      "9600 1.3510212681921985e-07\n",
      "9700 1.279234567164167e-07\n",
      "9800 1.2899550938527682e-07\n",
      "9900 1.205521726888037e-07\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for t in range(10000):\n",
    "    # 우리의 함수를 적용하기 위해서는, Function.apply 메소드를 사용한다. 우리는 이것을 'relu'라고 부른다.\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # forward : 계산을 통한 y 예측\n",
    "    # 우리의 custom autograd operation을 사용하여 ReLU를 계산한다.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # loss 계산\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t%100 == 0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # backward를 위한 autograd 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient descent를 통한 weight 갱신\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # gradient 초기화\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        \n",
    "now = time.time() - since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.283974170684814"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
