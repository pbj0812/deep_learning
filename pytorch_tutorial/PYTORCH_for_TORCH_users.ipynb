{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH 사용자를 위한 PYTORCH 따라치기\n",
    "\n",
    "- Author : Soumith Chintala\n",
    "- 번역 : 박정환\n",
    "- 재구성 : 박범진\n",
    "- 원본 : [링크](https://tutorials.pytorch.kr/beginner/former_torchies_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. torch Tensor를 사용해보고 (Lua)Torch와의 차이 알아보기\n",
    "2. autograd 패키지 사용\n",
    "3. 신경망 구성\n",
    "  - 합성공 신경망 구성\n",
    "  - 순환 신경망 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.empty(5, 7, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8321, -0.5338, -1.3557, -1.3160,  1.1388,  1.9611, -0.3921],\n",
      "        [ 1.2003,  0.4142,  0.4677, -0.2806,  0.5839,  0.1989,  1.4824],\n",
      "        [-1.1681, -0.6514,  0.6338, -0.6966,  1.7018,  0.7653,  0.1988],\n",
      "        [-0.5449,  2.0696,  0.2691,  1.4259,  0.0579,  0.0491, -1.9611],\n",
      "        [-1.1346,  0.4757,  0.5323,  0.5826,  2.0657, -1.6675,  0.8987]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7, dtype=torch.double)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평균 0, 분산 1의 정규분포를 따르는 무작위 숫자로 double tensor 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IN-PLACE / OUT-OF-PLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
       "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
       "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
       "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
       "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.fill_(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.add(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0, 3] # select 1st row, 4th column from a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5000, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, 3:5] # selects all rows, 4th column and 5th column from a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000],\n",
      "        [3.5000, 3.5000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카멜표기법 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.empty(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.]])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.tensor([4, 0], dtype=torch.long), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- z의 첫 열을 x의 4번째 열과 더하고 z의 두 번째 열을 x의 0번째 열과 더함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOGRAD\n",
    "\n",
    "- 자동 미분을 위해 tape 기반 시스템 사용\n",
    "- 순전파 단계에서 tape는 수행하는 모뎐 연산을 기억하고 역전파 단계에서 연산들을 재생함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이력을 추적하는 TENSOR\n",
    "\n",
    "- requires_grad=True로 설정된 Tensor의 연산은 기록\n",
    "- 역전파 단계 연산 후에, 이 변수에 대한 변화도는 .grad에 누적\n",
    "- 미분을 계산하기 위해서는 Tensor의 .backward() 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용자가 만든 Tensor(x)는 grad_fn이 None으로 지정\n",
    "- 연산의 결과로 생성된 y는 grad_fn을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x11f838390>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a*3) / (a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변화도(GRADIENT)\n",
    "\n",
    "- d/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 부분에 대한 역전파 연산을 2번하고 싶다면, 첫 연산 단계에서 retain_variables = True 값을 넘겨줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "y.backward(torch.ones(2, 2), retain_graph=True)\n",
    "# retain_variables flag는 내부 버퍼가 사라지는 것을 막아줌\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<ThMulBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4620,  0.5246],\n",
      "        [ 1.6958, -1.0808]])\n"
     ]
    }
   ],
   "source": [
    "gradient = torch.randn(2, 2)\n",
    "y.backward(gradient)\n",
    "# retain_variable을 지정하지 않으면 오류 발생\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 추적 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상태는 모듈 내에 저장되지 않고, 신경망 그래프 상에 존재\n",
    "- 동일한 linear layer를 여러 차례 호출하면 순환신경망이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![torch pytorch 비교](https://tutorials.pytorch.kr/_images/torch-nn-vs-pytorch-nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What you see is what you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제1 : 합성곱 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        # 어떻게 저장되는지는 아래의 '신경망 사용' 참고\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MNISTConvNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.nn은 미니 배치만 지원\n",
    "- nnConv2D는 nSamples x nChannels x Height x Width의 4차원 Tensor를 입력으로 함\n",
    "- 하나의 샘플만 있을 경우 input.unsqueeze(0)을 사용해서 가짜 차원 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0805, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([3], dtype = torch.long)\n",
    "loss_fn = nn.CrossEntropyLoss() # LogSoftmax + ClassNLL Loss\n",
    "err = loss_fn(out, target)\n",
    "err.backward()\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.grad.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개별 계층의 weight와 gradient에 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8393)\n",
      "tensor(0.7516)\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.data.norm()) # norm of the weight\n",
    "print(net.conv1.weight.grad.data.norm()) # norm of the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순방향/역방향 함수 훅(HOOK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "input:  <class 'tuple'>\n",
      "input[0]:  <class 'torch.Tensor'>\n",
      "output:  <class 'torch.Tensor'>\n",
      "\n",
      "input size:  torch.Size([1, 10, 12, 12])\n",
      "output size:  torch.Size([1, 20, 8, 8])\n",
      "output norm:  tensor(13.7558)\n"
     ]
    }
   ],
   "source": [
    "def printnorm(self, input, output):\n",
    "    # input is a tuple of packed inputs\n",
    "    # output is a Tensor. output.data is the Tensor we are interested\n",
    "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "    print('')\n",
    "    print('input: ', type(input))\n",
    "    print('input[0]: ', type(input[0]))\n",
    "    print('output: ', type(output))\n",
    "    print('')\n",
    "    print('input size: ', input[0].size())\n",
    "    print('output size: ', output.data.size())\n",
    "    print('output norm: ', output.data.norm())\n",
    "    \n",
    "    \n",
    "net.conv2.register_forward_hook(printnorm)\n",
    "\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "input:  <class 'tuple'>\n",
      "input[0]:  <class 'torch.Tensor'>\n",
      "output:  <class 'torch.Tensor'>\n",
      "\n",
      "input size:  torch.Size([1, 10, 12, 12])\n",
      "output size:  torch.Size([1, 20, 8, 8])\n",
      "output norm:  tensor(13.7558)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.backward of tensor(2.0805, grad_fn=<NllLossBackward>)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "    print('Inside class: ' + self.__class__.__name__)\n",
    "    print('')\n",
    "    print('grad_input: ', type(grad_input))\n",
    "    print('grad_input[0]: ', type(grad_input[0]))\n",
    "    print('grad_output: ', type(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]))\n",
    "    print('')\n",
    "    print('grad_input size: ', grad_input[0].size())\n",
    "    print('grad_output size: ', grad_output[0].size())\n",
    "    print('grad_input norm: ', grad_input[0].norm())\n",
    "    \n",
    "net.conv2.register_backward_hook(printgradnorm)\n",
    "\n",
    "out = net(input)\n",
    "err = loss_fn(out, target)\n",
    "err.backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제2 : 순환 신경망(RECURRENT NETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    # you can also accept arguments in your model constructor\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = data_size + hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "        hidden = self.i2h(input)\n",
    "        output = self.h2o(hidden)\n",
    "        return hidden, output\n",
    "    \n",
    "rnn = RNN(50, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (i2h): Linear(in_features=70, out_features=20, bias=True)\n",
       "  (h2o): Linear(in_features=20, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "batch_size = 10\n",
    "TIMESTEPS = 5\n",
    "\n",
    "# Create some fake data\n",
    "batch = torch.randn(batch_size, 50)\n",
    "hidden = torch.zeros(batch_size, 20)\n",
    "target = torch.zeros(batch_size, 10)\n",
    "\n",
    "loss = 0\n",
    "for t in range(TIMESTEPS):\n",
    "    hidden, output = rnn(batch, hidden)\n",
    "    loss += loss_fn(output, target)\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
