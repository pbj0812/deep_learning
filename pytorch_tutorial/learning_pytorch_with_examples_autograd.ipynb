{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING PYTORCH WITH EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 : [링크](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "- Author : [Justin Johnson](https://github.com/jcjohnson/pytorch-examples)\n",
    "- 번역 & 주석 추가 : [박범진](https://github.com/pbj0812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensors and autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제(tensor)에서, 우리는 뉴럴 네트워크의 forward와 backward에 대한 설계를 직접 해줘야 했다. 작은 two-layer 구성의 네트워크에서 직접 설계하는 것은 힘든 일은 아니지만, 복잡하고 큰 네트워크에서는 쉬운 일이 아니다.\n",
    "\n",
    "다행스럽게도, 우리는 네트워크에서의 backward 연산을 자동 미분을 이용하여 계산할 수 있다. PyTorch에서 autograd 패키지는 정확하게 이 기능을 제공하고 있다. autograd를 사용할 때, 네트워크의 forward 연산은 computational graph에 정의된다; 그래프에서 node는 Tensor가 되고, edge는 입력 Tensor에서 생산되는 출력 Tensor에 대한 함수가 된다. 이 그래프에 의한 backpropagating은 gradients의 연산을 쉽게 만들어 준다.\n",
    "\n",
    "이 말은 복잡하게 들리기에, 간단히 예를 들어본다. 각각의 Tensor는 computational graph에서의 노드로 표현된다. 만약 x Tensor가 x.requires_grad=True를 가지고 있을 경우 x.grad는 x의 gradient를 가지게 된다.\n",
    "\n",
    "이번에 우리는 PyTorch Tensor와 autograd를 사용하여 two-layer로 이루어진 네트워크를 구현해본다; 이제 우리는 더이상 network내의 backward 연산에 대한 직접적인 구현은 하지 않아도 된다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\") # GPU 사용 불가시 주석을 푼다.\n",
    "device = torch.device(\"cuda:0\") # GPU 사용 가능시 주석을 푼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# input과 output을 가지는 random한 Tensor 생성\n",
    "# requires_grad=False 는 Tensor에 대한 backward 연산시 gradients를 계산하지 않는 옵션이다.\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# weight에 대한 random한 Tensor 생성\n",
    "# requires_grad=True 는 Tensor에 대한 backward 연산시 gradients를 계산하는 옵션이다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33383432.0\n",
      "100 822.190673828125\n",
      "200 8.993412971496582\n",
      "300 0.18210481107234955\n",
      "400 0.004567751660943031\n",
      "500 0.0003030354855582118\n",
      "600 6.236392800929025e-05\n",
      "700 2.5421461032237858e-05\n",
      "800 1.5219056876958348e-05\n",
      "900 1.062655974237714e-05\n",
      "1000 8.182493729691487e-06\n",
      "1100 6.5120411818497814e-06\n",
      "1200 5.386174962040968e-06\n",
      "1300 4.668338078772649e-06\n",
      "1400 4.064198037667666e-06\n",
      "1500 3.5892840060114395e-06\n",
      "1600 3.183968601661036e-06\n",
      "1700 2.9025666208326584e-06\n",
      "1800 2.6311072360840626e-06\n",
      "1900 2.39645646615827e-06\n",
      "2000 2.2050201096135424e-06\n",
      "2100 2.044190750893904e-06\n",
      "2200 1.922990804814617e-06\n",
      "2300 1.7555100839672377e-06\n",
      "2400 1.6076536439868505e-06\n",
      "2500 1.5479515695915325e-06\n",
      "2600 1.4235063190426445e-06\n",
      "2700 1.3222905863585765e-06\n",
      "2800 1.248449734703172e-06\n",
      "2900 1.1970320201726281e-06\n",
      "3000 1.143174358730903e-06\n",
      "3100 1.0892467798839789e-06\n",
      "3200 1.0330104487366043e-06\n",
      "3300 9.901873454509769e-07\n",
      "3400 9.474897524341941e-07\n",
      "3500 9.101531759370118e-07\n",
      "3600 8.716738193470519e-07\n",
      "3700 8.510766065228381e-07\n",
      "3800 8.068676606853842e-07\n",
      "3900 7.624665840921807e-07\n",
      "4000 7.466845204362471e-07\n",
      "4100 6.870936317682208e-07\n",
      "4200 6.807128443142574e-07\n",
      "4300 6.767573381694092e-07\n",
      "4400 6.462209398705454e-07\n",
      "4500 6.106218393142626e-07\n",
      "4600 6.069496407690167e-07\n",
      "4700 6.00028272401687e-07\n",
      "4800 5.728849714614626e-07\n",
      "4900 5.490117587214627e-07\n",
      "5000 5.504153932633926e-07\n",
      "5100 5.131227567289898e-07\n",
      "5200 4.951198206981644e-07\n",
      "5300 4.7675791847723303e-07\n",
      "5400 4.686899899297714e-07\n",
      "5500 4.626279803687794e-07\n",
      "5600 4.4351023120725586e-07\n",
      "5700 4.3870574018001207e-07\n",
      "5800 4.3206543409723963e-07\n",
      "5900 4.2753072193590924e-07\n",
      "6000 4.066988026352192e-07\n",
      "6100 3.9395283124576963e-07\n",
      "6200 3.931784249289194e-07\n",
      "6300 3.798023158196884e-07\n",
      "6400 3.825425096692925e-07\n",
      "6500 3.7498776350730623e-07\n",
      "6600 3.7806074715263094e-07\n",
      "6700 3.598817954753031e-07\n",
      "6800 3.6648046375376e-07\n",
      "6900 3.519548670283257e-07\n",
      "7000 3.3689480005705263e-07\n",
      "7100 3.2518184411856055e-07\n",
      "7200 3.3451789249738795e-07\n",
      "7300 3.264116230639047e-07\n",
      "7400 3.1309852488448087e-07\n",
      "7500 3.165935140714282e-07\n",
      "7600 3.143573792385723e-07\n",
      "7700 3.054859973872226e-07\n",
      "7800 2.916306414135761e-07\n",
      "7900 2.898789546179614e-07\n",
      "8000 2.8967895104869967e-07\n",
      "8100 2.9220001351859537e-07\n",
      "8200 2.833816097336239e-07\n",
      "8300 2.7478350261844753e-07\n",
      "8400 2.6948197273668484e-07\n",
      "8500 2.614568472836254e-07\n",
      "8600 2.552213516082702e-07\n",
      "8700 2.459727852510696e-07\n",
      "8800 2.4241955998149933e-07\n",
      "8900 2.436381123516185e-07\n",
      "9000 2.3176632168997457e-07\n",
      "9100 2.3871626808613655e-07\n",
      "9200 2.3607448440543521e-07\n",
      "9300 2.3266726145720895e-07\n",
      "9400 2.270515722102573e-07\n",
      "9500 2.2656404041754286e-07\n",
      "9600 2.2271159139108931e-07\n",
      "9700 2.1814089734562003e-07\n",
      "9800 2.2140197586395516e-07\n",
      "9900 2.186692142913671e-07\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for t in range(10000):\n",
    "    # Forward pass 는 기존의 방식과 같지만, backward 연산에 대해서는 손으로 풀지 않아도 된다.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Tensor를 통한 loss 연산\n",
    "    # loss의 형태는 (1,)\n",
    "    # loss.item()은 loss에서의 스칼라 값을 가진다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t%100 == 0:\n",
    "        print(t, loss.item())\n",
    "    # backward 연산을 위한 autograd 사용. 이 구문은 requires_grad=True라고 지정된 모든 Tensor에 대한 \n",
    "    # loss의 gradient를 계산한다.\n",
    "    # 이후 w1.grad와 w2.grad는 w1와 w2 각각에 대한 loss의 gradient를 가진다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # 수동으로 weight를 갱신하기 위하여 torch.no_grad()를 사용한다.\n",
    "    # 왜냐하면 weight들은 requires_grad=True를 가지지만, 우리는 autograd 내에서 추적할 필요가 없기 때문이다.\n",
    "    # 대체적인 방법으로는 weight.data와 weight.grad.data.를 사용하는 방법이 있다.\n",
    "    # tensor.data는 tensor를 저장하는 저장소를 제공하지만, 이력을 추적하지는 않는다.\n",
    "    # torch.optim.SGD 를 사용할 수도 있다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # weight 갱신후 수동 초기화\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        \n",
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.585309267044067\n"
     ]
    }
   ],
   "source": [
    "print(now - since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
